{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bca0536-9b58-4bdf-8324-f4e73528d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sunpy.map\n",
    "from sunpy.time import parse_time\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "import astropy.stats\n",
    "import math\n",
    "import os\n",
    "import threading\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "import sys\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import random\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "sys.path.append(os.path.join(Path.cwd(), 'utils'))\n",
    "\n",
    "from utils.im_utils import *\n",
    "from utils.data_augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3feb98fe-b71a-4f0e-8262-1192c1ec4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd187670-4bb0-489c-a689-570c9a6abd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLARE_CLASS = 'ALL'\n",
    "ANGSTROM = '0094'        # 0094/1600/bz\n",
    "EUV_CHANNEL = 'AIA'      # AIA/HMI\n",
    "\n",
    "AIA_DATA_DIR = '../data_94' # ../data or ../data/data_1600 or ../data/data_HMI_bz\n",
    "\n",
    "EVENTS_BY_DATE_DIR = './event_records/new_events_by_date'\n",
    "EVENTS_BY_CLASS_DIR = './event_records/new_events_by_class'\n",
    "\n",
    "TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data/train/positive'\n",
    "TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data/train/negative'\n",
    "TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data/test/positive'\n",
    "TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data/test/negative'\n",
    "VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data/val/positive'\n",
    "VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data/val/negative'\n",
    "\n",
    "AUG_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/train/positive'\n",
    "AUG_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/train/negative'\n",
    "AUG_TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/test/positive'\n",
    "AUG_TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/test/negative'\n",
    "AUG_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/val/positive'\n",
    "AUG_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/val/negative'\n",
    "\n",
    "AUG_PAIR_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/train/positive'\n",
    "AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/train/negative'\n",
    "AUG_PAIR_TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/test/positive'\n",
    "AUG_PAIR_TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/test/negative'\n",
    "AUG_PAIR_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/val/positive'\n",
    "AUG_PAIR_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/val/negative'\n",
    "\n",
    "LSTM_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/train/positive'\n",
    "LSTM_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/train/negative'\n",
    "LSTM_TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/test/positive'\n",
    "LSTM_TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/test/negative'\n",
    "LSTM_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/val/positive'\n",
    "LSTM_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/val/negative'\n",
    "\n",
    "LSTM_TRAIN_DATA_EXT_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/train/positive'\n",
    "LSTM_TRAIN_DATA_EXT_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/train/negative'\n",
    "LSTM_TEST_DATA_EXT_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/test/positive'\n",
    "LSTM_TEST_DATA_EXT_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/test/negative'\n",
    "LSTM_VAL_DATA_EXT_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/val/positive'\n",
    "LSTM_VAL_DATA_EXT_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/val/negative'\n",
    "\n",
    "LSTM_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/train/positive'\n",
    "LSTM_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/train/negative'\n",
    "LSTM_AUG_PAIR_TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/test/positive'\n",
    "LSTM_AUG_PAIR_TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/test/negative'\n",
    "LSTM_AUG_PAIR_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/val/positive'\n",
    "LSTM_AUG_PAIR_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/val/negative'\n",
    "\n",
    "LSTM_AUG_ALL_CLASS_TRAIN_DATA_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented/train/'\n",
    "LSTM_AUG_ALL_CLASS_VAL_DATA_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented/val/'\n",
    "\n",
    "LSTM_ALL_CLASS_PRIOR_DATA_DIR = f'./new_data/{FLARE_CLASS}_lstm_data_prior'\n",
    "LSTM_ALL_CLASS_DURING_DATA_DIR = f'./new_data/{FLARE_CLASS}_lstm_data_during'\n",
    "LSTM_ALL_CLASS_LATESTART_DATA_DIR = f'./new_data/{FLARE_CLASS}_lstm_data_latestart'\n",
    "LSTM_ALL_CLASS_PRIOR_LEFTOUT2013_DATA_DIR = f'./new_data/{FLARE_CLASS}_lstm_data_prior_leftout2013'\n",
    "LSTM_ALL_CLASS_POST_LEFTOUT2013_DATA_DIR = f'./new_data/{FLARE_CLASS}_lstm_data_post_leftout2013'\n",
    "\n",
    "LSTM_END_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_end_data_augmented_pair/train/positive'\n",
    "LSTM_END_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_end_data_augmented_pair/train/negative'\n",
    "LSTM_END_AUG_PAIR_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_end_data_augmented_pair/val/positive'\n",
    "LSTM_END_AUG_PAIR_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_end_data_augmented_pair/val/negative'\n",
    "\n",
    "PAIR_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_full_image_data/train/positive'\n",
    "PAIR_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_full_image_data/train/negative'\n",
    "PAIR_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_full_image_data/val/positive'\n",
    "PAIR_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_full_image_data/val/negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30a8dba5-c70d-4ba6-8360-e714efacdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDELT = 0.599733;\n",
    "HPCCENTER = 4096.0 / 2.0;\n",
    "rsun_meters = 696000;\n",
    "dsun_meters = 149600000;\n",
    "DEFAULT_WIDTH, DEFAULT_HEIGHT = 64, 64\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 512, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ec4f53d-afbe-4345-8528-225b5c1ec0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e21c0ce-f00e-46c1-aa8c-1b0df00f5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_data_dir(data_dir):\n",
    "    delete_files(os.path.join(data_dir, 'train'))\n",
    "    delete_files(os.path.join(data_dir, 'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd7a971e-3b4f-42cd-8052-ffff85ce4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateAugmentedLSTMData(paths, coord, sample_mult):\n",
    "    full_images = [np.load(im)['x'] for im in paths]\n",
    "    large_cutouts = []\n",
    "    for full_im in full_images:\n",
    "        large_cutouts.append(GetSafeCenteredCutout(full_im, 128, coord))\n",
    "        \n",
    "    rot_data = np.array([GenerateRotateData(im, sample_mult, 64) for im in large_cutouts])\n",
    "    rot_data_arranged = []\n",
    "    for i in range(rot_data.shape[0]):\n",
    "        rot_data_arranged.append([rot_data[i][x] for x in range(rot_data.shape[1])])\n",
    "    rot_data_arranged = np.array(rot_data_arranged)\n",
    "    \n",
    "    return rot_data_arranged#np.concatenate((rot_data, flip_data, zoom_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84860f29-f42d-45d1-b64f-4b6ea566df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "num = 0\n",
    "for subdir, dirs, files in os.walk(f'./data/M_lstm_data_augmented_pair/train/positive/AIA20111031_1721_0094/20/sequence'):\n",
    "    for f in files:\n",
    "        data.append(os.path.join(subdir, f))\n",
    "full_data = []\n",
    "for subdir, dirs, files in os.walk(f'./data/M_lstm_data_augmented_pair/train/positive/AIA20111031_1721_0094/0/full'):\n",
    "    for f in files:\n",
    "        full_data.append(os.path.join(subdir, f))\n",
    "data = [np.load(x) for x in data]\n",
    "full_data = [np.load(x) for x in full_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9042ba17-62c4-40cb-828d-17e666b360c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateNoFlareCSV(csv_to_copy):\n",
    "    df = pd.read_csv(csv_to_copy)\n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    for index, row in copy_df.iterrows():\n",
    "        old_starttime, old_endtime = parse_time(row['event_starttime'], precision=0), parse_time(row['event_endtime'], precision=0)\n",
    "        new_starttime, new_endtime = old_starttime + datetime.timedelta(hours=-12), old_endtime + datetime.timedelta(hours=-12)\n",
    "        copy_df.at[index,'event_starttime'] = new_starttime\n",
    "        copy_df.at[index,'event_endtime'] = new_endtime\n",
    "        copy_df.at[index,'fl_goescls'] = 'N0.0'\n",
    "    \n",
    "    copy_df.to_csv('./event_records/new_events_by_class/N.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78936ffd-91fd-4467-9a6f-036117788964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateNoFlareCSVByYears(years, samples, non_flare_duration=72):\n",
    "    no_flare_df = pd.DataFrame(columns=['event_starttime','event_endtime','fl_goescls','hpc_x','hpc_y'])\n",
    "    for year in years:\n",
    "        print(f'processing year {year}')\n",
    "        month_csv_paths = []\n",
    "        for subdir, dirs, files in os.walk(f'./event_records/new_events_by_date/{year}'):\n",
    "            for f in files:\n",
    "                month_csv_paths.append(os.path.join(subdir, f))\n",
    "            break\n",
    "         \n",
    "        for month_path in month_csv_paths:\n",
    "            month = month_path.rsplit('.', 1)[0].rsplit('/')[-1]\n",
    "            # print(f'processing month {month}')\n",
    "            month_df = pd.read_csv(month_path)\n",
    "            days_with_flares = set()\n",
    "            \n",
    "            for index, row in month_df.iterrows():\n",
    "                event_day = parse_time(row['event_starttime'], precision=0).datetime.day\n",
    "                days_with_flares.add(event_day)\n",
    "            min_day, max_day = min(days_with_flares), max(days_with_flares)\n",
    "            all_days = range(min_day, max_day+1)\n",
    "            days_without_flares = set()\n",
    "            days_without_flares.update(all_days)\n",
    "            days_without_flares -= days_with_flares\n",
    "            print(f'days without flares year {year} month {month} - {days_without_flares}')\n",
    "\n",
    "            for day in days_without_flares:\n",
    "                day_start_datetime = datetime.datetime(int(year), int(month), int(day))\n",
    "                dynamic_time = day_start_datetime\n",
    "                while(dynamic_time) < day_start_datetime + datetime.timedelta(hours=24):\n",
    "                    no_flare_df = pd.concat([no_flare_df, pd.DataFrame.from_records([{\n",
    "                            'event_starttime': dynamic_time, \n",
    "                            'event_endtime': dynamic_time+datetime.timedelta(minutes=non_flare_duration), \n",
    "                            'fl_goescls': 'N0.0',\n",
    "                            'hpc_x': 100,\n",
    "                            'hpc_y': 100,\n",
    "                        }])])\n",
    "                    \n",
    "                    dynamic_time += datetime.timedelta(minutes=non_flare_duration)\n",
    "    no_flare_df.to_csv('./event_records/no_flares.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebded59a-185b-4801-b6ae-3055addd9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateHybridCSV():\n",
    "    df_C = pd.read_csv('./event_records/new_events_by_class/C.csv')\n",
    "    df_N = pd.read_csv('./event_records/new_events_by_class/N.csv')\n",
    "    df_H = pd.concat([df_C, df_N])\n",
    "    \n",
    "    for index, row in df_H.iterrows():\n",
    "        df_H.at[index, 'fl_goescls'] = 'H0.0'\n",
    "    \n",
    "    df_H.to_csv('./event_records/new_events_by_class/H.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2679576e-00bd-41c0-b166-c02d84c79121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAIAFormatFilename(dt):\n",
    "    AIA_data_date = f'{dt.year}{dt.month:02d}{dt.day:02d}'\n",
    "    AIA_data_time = f'{dt.hour:02d}{dt.minute:02d}'\n",
    "    AIA_data_filename = f'{EUV_CHANNEL}{AIA_data_date}_{AIA_data_time}_{ANGSTROM}'\n",
    "    \n",
    "    return AIA_data_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a3f3284-f780-4ee3-910d-3ea5d527eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a formatted file name of the closest AIA data file to the given datetime\n",
    "\n",
    "def GetClosestDataFileByDate(dt, rounding):\n",
    "    AIA_data_date = f'{dt.year}{dt.month:02d}{dt.day:02d}'\n",
    "    tmp_dt = dt\n",
    "    minute = 0\n",
    "    minute=GetClosestMultiple(tmp_dt.minute, 6)\n",
    "    AIA_data_time = f'{tmp_dt.hour:02d}{minute:02d}'\n",
    "    AIA_data_filename = f'{EUV_CHANNEL}{AIA_data_date}_{AIA_data_time}_{ANGSTROM}.npz'\n",
    "    \n",
    "    return AIA_data_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b40db44-68bd-452f-9e7b-43c97ccd768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCutout(im, coord, N=64):\n",
    "    x_end_idx = int(coord[0]+N)\n",
    "    y_end_idx = int(coord[1]+N)\n",
    "    cutout_array = im[int(coord[0]):x_end_idx, int(coord[1]):y_end_idx]\n",
    "    \n",
    "    return cutout_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc4213e6-922b-4710-b692-eb96b2e8d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCenteredCutout(im, coord, N=64):\n",
    "    x, y = int(coord[0]), int(coord[1])\n",
    "    offset = N//2\n",
    "    x_start, x_end = x-offset, x+offset\n",
    "    y_start, y_end = y-offset, y+offset\n",
    "    cutout_array = im[x_start:x_end, y_start:y_end]\n",
    "    \n",
    "    return cutout_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fed19cb9-2a54-4041-9d38-d6f33624bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an NxN cutout of a file centered around the given coordinate\n",
    "\n",
    "def GetFileCutout(path, coord, N=64):\n",
    "    im = np.load(path)['x']\n",
    "    # scs = astropy.stats.sigma_clipped_stats(im)\n",
    "    # mean, median, sd = scs[0], scs[1], scs[2]\n",
    "    # thr = median+sd*3\n",
    "    # im[im<thr]=0\n",
    "    \n",
    "    return GetCutout(im, coord, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4529c69-de72-4dba-b80a-c9d091e16e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an AIA filepath closest to the provided\n",
    "\n",
    "def GetAIAPathAtTime(dt):\n",
    "    dt_data_dir = os.path.join(AIA_DATA_DIR, f'{dt.year}/{dt.month:02d}/{dt.day:02d}')\n",
    "    closest_data_file = GetClosestDataFileByDate(dt, \"up\")\n",
    "    file_path = os.path.join(dt_data_dir, closest_data_file)\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebc2173d-c22d-47a2-b272-a678598cd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedCutouts(coord, path, save_path, rotation_step=12):\n",
    "    # create augmented cutouts which are rotated, flipped, and mirrored\n",
    "    cutout =  GetFileCutout(path, coord, N=128)\n",
    "    return cutout\n",
    "    cur_rot = rotation_step\n",
    "    for i in range(360//rotation_step):\n",
    "        rot_image = rotate(cutout, 0)\n",
    "        rot_cutout = GetCenteredCutout(rot_image, (rot_image.shape[0]//2, rot_image.shape[1]//2), 64)\n",
    "        cur_rot += rotation_step\n",
    "        return rot_cutout\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0847ac90-0c55-4bf7-933a-071152537177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# returns M consecutive (forward or backward) AIA filepaths starting at time closest to the given time\n",
    "\n",
    "def GetAIANCutoutsPaths(start_dt, direction='backward', M=6, cadence=6):\n",
    "    cutouts_paths = []\n",
    "    if direction == 'backward':\n",
    "        cadence = -cadence\n",
    "    dynamic_dt = start_dt\n",
    "        \n",
    "    for i in range(M):\n",
    "        try:\n",
    "            cutout_path = GetAIAPathAtTime(dynamic_dt)\n",
    "        except FileNotFoundError:\n",
    "            dynamic_dt = dynamic_dt + datetime.timedelta(minutes=cadence)\n",
    "            continue\n",
    "        cutouts_paths.append(cutout_path)\n",
    "        dynamic_dt = dynamic_dt + datetime.timedelta(minutes=cadence)\n",
    "    \n",
    "    cutouts_paths = sorted(cutouts_paths)\n",
    "    num_paths = len(cutouts_paths)\n",
    "    filled_paths = cutouts_paths\n",
    "\n",
    "    if num_paths != M:\n",
    "        diff = abs(M-num_paths)\n",
    "        for i in range(num_paths, num_paths-diff, -1):\n",
    "            filled_paths.insert(i-1, filled_paths[i-1])\n",
    "        filled_paths = sorted(filled_paths)\n",
    "    filled_paths = sorted(filled_paths)\n",
    "    return np.array(filled_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2929b5a-f5c4-4eab-b2a8-0198df8947a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIANCutoutsFromDF(df, save_dir, time_delta, num_frames=1, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0]-32, 512-coord[1]-32)\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "        \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "        \n",
    "        if len(paths) > 1:\n",
    "            folder_name = GetAIAFormatFilename(start_dt)\n",
    "            event_folder_path = os.path.join(save_dir, folder_name)\n",
    "            if not os.path.exists(event_folder_path):\n",
    "                os.makedirs(event_folder_path)\n",
    "            else:\n",
    "                print(f'trying to create existing folder, time: {start_dt}')\n",
    "                continue \n",
    "            mod_save_dir = event_folder_path\n",
    "        for idx, p in enumerate(paths):\n",
    "            cutout = GetFileCutout(p, coord)\n",
    "            save_filename = os.path.basename(p).rsplit('.', 1)[0]\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_{idx}', cutout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68d6890a-6c8e-4505-a38d-eb008d6b7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIAPairFromDF(df, save_dir, time_delta, num_frames=1, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0]-32, 512-coord[1]-32)\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        # full sun image path\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "        \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "            \n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(save_dir, folder_name)\n",
    "        if not os.path.exists(event_folder_path):\n",
    "            os.makedirs(event_folder_path)\n",
    "        else:\n",
    "            print(f'trying to create existing folder, time: {start_dt}')\n",
    "            continue \n",
    "        mod_save_dir = event_folder_path\n",
    "        for idx, p in enumerate(paths):\n",
    "            cutout = GetFileCutout(p, coord)\n",
    "            save_filename = os.path.basename(p).rsplit('.', 1)[0]\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_{idx}', cutout)\n",
    "            \n",
    "            full_image = np.load(closest_file_path)['x']\n",
    "            full_image = cv2.resize(full_image, (128, 128), interpolation = cv2.INTER_AREA)\n",
    "            scs = astropy.stats.sigma_clipped_stats(full_image)\n",
    "            mean, median, sd = scs[0], scs[1], scs[2]\n",
    "            thr = median+sd*3\n",
    "            full_image[full_image<thr]=0\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_full', full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2527d7c-b7e6-4385-af98-3c3d63b76aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIAPairFromDF(df, save_dir, time_delta, num_frames=1, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0]-32, 512-coord[1]-32)\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        # full sun image path\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "        \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "            \n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(save_dir, folder_name)\n",
    "        if not os.path.exists(event_folder_path):\n",
    "            os.makedirs(event_folder_path)\n",
    "        else:\n",
    "            print(f'trying to create existing folder, time: {start_dt}')\n",
    "            continue \n",
    "        mod_save_dir = event_folder_path\n",
    "        for idx, p in enumerate(paths):\n",
    "            cutout = GetFileCutout(p, coord)\n",
    "            save_filename = os.path.basename(p).rsplit('.', 1)[0]\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_{idx}', cutout)\n",
    "            \n",
    "            full_image = np.load(closest_file_path)['x']\n",
    "            full_image = cv2.resize(full_image, (128, 128), interpolation = cv2.INTER_AREA)\n",
    "            scs = astropy.stats.sigma_clipped_stats(full_image)\n",
    "            mean, median, sd = scs[0], scs[1], scs[2]\n",
    "            thr = median+sd*3\n",
    "            full_image[full_image<thr]=0\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_full', full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af2a239f-4a6c-4ef2-8a62-634b4988c0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SaveAIAAugmentedDataFromDF(df, save_dir, time_delta, num_frames=1, sample_mult=60, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0], 512-coord[1])\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "        \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "        \n",
    "        for idx, p in enumerate(paths):\n",
    "            full_image = np.load(closest_file_path)['x']\n",
    "            aug_data = GenerateAugmentedData(full_image, coord, sample_mult)\n",
    "            aug_data = np.concatenate((aug_data))\n",
    "            for idx, d in enumerate(aug_data):\n",
    "                folder_name = GetAIAFormatFilename(start_dt)\n",
    "                event_folder_path = f'{os.path.join(save_dir, folder_name)}_{idx}'\n",
    "                if not os.path.exists(event_folder_path):\n",
    "                    os.makedirs(event_folder_path)\n",
    "                else:\n",
    "                    print(f'trying to create existing folder, time: {start_dt}')\n",
    "                    continue \n",
    "                mod_save_dir = event_folder_path\n",
    "                save_filename = os.path.basename(p).rsplit('.', 1)[0]\n",
    "                np.save(f'{mod_save_dir}/{save_filename}_augmented_{idx}', d)\n",
    "                np.save(f'{mod_save_dir}/{save_filename}_full', full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d0e50dc-5943-44b2-8ff6-3002a14872d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIANAugmentedDataFromDF(df, save_dir, time_delta, positive=True, num_frames=1, sample_mult=60, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        # if not positive:\n",
    "        #     time_delta = -18\n",
    "            # time_delta = random.choice([-36, 12])\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0], 512-coord[1])\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "            \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        paths = sorted(paths)\n",
    "            \n",
    "        aug_data = GenerateAugmentedLSTMData(paths, coord, sample_mult)\n",
    "        full_images = [np.load(im)['x'] for im in paths]\n",
    "        full_image_aug_data = np.array([GenerateRotateData(im, sample_mult, 512) for im in full_images])\n",
    "\n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(save_dir, folder_name)\n",
    "        for i in range(aug_data.shape[1]):\n",
    "            subfolder_path = os.path.join(event_folder_path, str(i))\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.makedirs(subfolder_path)\n",
    "            sequence_folder_path = os.path.join(subfolder_path, 'sequence')\n",
    "            if not os.path.exists(sequence_folder_path):\n",
    "                os.makedirs(sequence_folder_path)\n",
    "            full_images_folder_path = os.path.join(subfolder_path, 'full')\n",
    "            if not os.path.exists(full_images_folder_path):\n",
    "                os.makedirs(full_images_folder_path)\n",
    "            for j in range(aug_data.shape[0]):\n",
    "                aia_filename = paths[j].rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "                save_filename = f'{aia_filename}_{j}'\n",
    "                np.save(f'{sequence_folder_path}/{save_filename}', aug_data[j][i])\n",
    "                np.save(f'{full_images_folder_path}/{save_filename}', full_image_aug_data[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "828653a2-38a4-4876-83ff-a301b2f3a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIAAllClassAugmentedDataFromDF(df, save_dir, time_delta, num_frames=1, sample_mult=None, direction='backward', starting_point='event_starttime', cadence=6):\n",
    "    # C: 6127 M: 612 X: 38\n",
    "    # multiplied by 2, 20, 320\n",
    "    # C: 12,254 M: 12,240 X: 12,160\n",
    "    print('started SaveAIAAllClassAugmentedDataFromDF')\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row[starting_point], precision=0), row['fl_goescls'][0]\n",
    "        cur_save_dir = os.path.join(save_dir, goes_cls)\n",
    "        if not os.path.exists(cur_save_dir):\n",
    "            os.makedirs(cur_save_dir)\n",
    "            \n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0], 512-coord[1])\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "        \n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "            \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=cadence)\n",
    "        \n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "        \n",
    "        if sample_mult == None:\n",
    "            sample_mult = math.ceil(10000 / len(df))\n",
    "        aug_data = GenerateAugmentedLSTMData(paths, coord, sample_mult)\n",
    "        full_images = [np.load(im)['x'] for im in paths]\n",
    "        full_image_aug_data = np.array([GenerateRotateData(im, sample_mult, 512) for im in full_images])\n",
    "\n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(cur_save_dir, folder_name)\n",
    "        for i in range(aug_data.shape[1]):\n",
    "            subfolder_path = os.path.join(event_folder_path, str(i))\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.makedirs(subfolder_path)\n",
    "            sequence_folder_path = os.path.join(subfolder_path, 'sequence')\n",
    "            if not os.path.exists(sequence_folder_path):\n",
    "                os.makedirs(sequence_folder_path)\n",
    "            full_images_folder_path = os.path.join(subfolder_path, 'full')\n",
    "            if not os.path.exists(full_images_folder_path):\n",
    "                os.makedirs(full_images_folder_path)\n",
    "            for j in range(aug_data.shape[0]):\n",
    "                aia_filename = paths[j].rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "                save_filename = f'{aia_filename}_{j}'\n",
    "                np.save(f'{sequence_folder_path}/{save_filename}', aug_data[j][i])\n",
    "                np.save(f'{full_images_folder_path}/{save_filename}', full_image_aug_data[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "478b484d-2eb1-4ea1-a66f-eb74b08429de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIABinaryAugmentedDataFromDF(df, save_dir, time_delta, num_frames=1, sample_mult=None, direction='backward', starting_point='event_starttime', cadence=6):\n",
    "    # C: 6127 M: 612 X: 38\n",
    "    # multiplied by 2, 20, 320\n",
    "    # C: 12,254 M: 12,240 X: 12,160\n",
    "    \n",
    "    nonexistent = 0\n",
    "    pos_dir = os.path.join(save_dir, 'positive')\n",
    "    neg_dir = os.path.join(save_dir, 'negative')\n",
    "    if not os.path.exists(pos_dir):\n",
    "        os.makedirs(pos_dir)\n",
    "    if not os.path.exists(neg_dir):\n",
    "        os.makedirs(neg_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row[starting_point], precision=0), row['fl_goescls'][0]\n",
    "            \n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0], 512-coord[1])\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "            \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=cadence)\n",
    "        \n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "        \n",
    "        if sample_mult == None:\n",
    "            if goes_cls == 'N':\n",
    "                sample_mult = 40\n",
    "            elif goes_cls == 'C':\n",
    "                sample_mult = 1   \n",
    "            elif goes_cls == 'M':\n",
    "                sample_mult = 10\n",
    "            elif goes_cls == 'X':\n",
    "                sample_mult = 250\n",
    "        aug_data = GenerateAugmentedLSTMData(paths, coord, sample_mult)\n",
    "        full_images = [np.load(im)['x'] for im in paths]\n",
    "        full_image_aug_data = np.array([GenerateRotateData(im, sample_mult, 512) for im in full_images])\n",
    "\n",
    "        cur_save_dir = pos_dir\n",
    "        if goes_cls == 'N':\n",
    "            cur_save_dir = neg_dir\n",
    "        \n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(cur_save_dir, folder_name)\n",
    "        for i in range(aug_data.shape[1]):\n",
    "            subfolder_path = os.path.join(event_folder_path, str(i))\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.makedirs(subfolder_path)\n",
    "            sequence_folder_path = os.path.join(subfolder_path, 'sequence')\n",
    "            if not os.path.exists(sequence_folder_path):\n",
    "                os.makedirs(sequence_folder_path)\n",
    "            full_images_folder_path = os.path.join(subfolder_path, 'full')\n",
    "            if not os.path.exists(full_images_folder_path):\n",
    "                os.makedirs(full_images_folder_path)\n",
    "            for j in range(aug_data.shape[0]):\n",
    "                aia_filename = paths[j].rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "                save_filename = f'{aia_filename}_{j}'\n",
    "                np.save(f'{sequence_folder_path}/{save_filename}', aug_data[j][i])\n",
    "                np.save(f'{full_images_folder_path}/{save_filename}', full_image_aug_data[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed42af3f-a704-480a-af7c-6c4170a7ae54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateResnetData(test_split=0.25, val_split=0.15, classes='all'):\n",
    "    delete_data()\n",
    "    train_split = 1-test_split+val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateResnetDataOfClass(class_df, test_split, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ad92ff2-1778-42af-8bc5-b6deda2545e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateLSTMData(val_split=0.3, classes='all'):\n",
    "    delete_lstm_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateLSTMDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c697257f-406e-42a2-be37-daa5e36c7159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateLSTMPairData(val_split=0.3, classes='all'):\n",
    "    delete_lstm_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateLSTMPairDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e087239-d2c1-4ba7-9630-8468ce480624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreatePairData(val_split=0.3, classes='all'):\n",
    "    delete_pair_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreatePairDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0bbd696-14c1-4712-8ee2-4fd092f91aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateAugmentedData(val_split=0.3, classes='all'):\n",
    "    # delete_augmented_data()\n",
    "    delete_augmented_pair_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateAugmentedDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "422dd4b4-9b74-4242-9c37-632fa005cc1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMData(val_split=0.3, classes='all'):\n",
    "    # delete_augmented_data()\n",
    "    delete_lstm_augmented_pair_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateAugmentedLSTMDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "806e917b-fb6f-48ad-bf5a-9c11fdf15a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateEndAugmentedLSTMData(val_split=0.3, classes='all'):\n",
    "    delete_lstm_end_augmented_pair_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateEndAugmentedLSTMDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcb9ad15-1877-4f37-b2ba-210df99ac74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMDataAll(save_dir, direction, val_year, starting_point='event_starttime', classes=['M', 'X', 'N'], cadence=6, time_delta=0, num_frames=6):\n",
    "    reset_data_dir(save_dir)\n",
    "    event_files = []\n",
    "    \n",
    "    val_dfs = []\n",
    "    \n",
    "    p = Pool(8)\n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            event_files.append(os.path.join(subdir, file))\n",
    "    for file in event_files:\n",
    "        filename = file.rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "        if filename in classes:\n",
    "            print(file.rsplit('/', 1)[-1])\n",
    "            class_df = pd.read_csv(file)\n",
    "            \n",
    "            train_df = class_df\n",
    "            val_df = None\n",
    "            \n",
    "            if val_year != None:\n",
    "                class_df_pruned_rows = np.array([parse_time(x).datetime.year for x in class_df.event_starttime])\n",
    "                train_df = class_df[class_df_pruned_rows != val_year]\n",
    "                val_df = class_df[class_df_pruned_rows == val_year] \n",
    "            \n",
    "            p.apply_async(CreateAugmentedLSTMDataAllClass, (save_dir, train_df, val_df, direction, starting_point, cadence, time_delta, num_frames))\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74e81bf9-da23-4b70-9c0c-33b2f953bd70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMDataBinary(save_dir, direction, val_year, starting_point='event_starttime', classes=['M', 'X', 'N'], cadence=6, time_delta=0):\n",
    "    reset_data_dir(save_dir)\n",
    "    event_files = []\n",
    "    \n",
    "    val_dfs = []\n",
    "    \n",
    "    p = Pool(8)\n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            event_files.append(os.path.join(subdir, file))\n",
    "    for file in event_files:\n",
    "        filename = file.rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "        if filename in classes:\n",
    "            print(file.rsplit('/', 1)[-1])\n",
    "            class_df = pd.read_csv(file)\n",
    "            \n",
    "            train_df = class_df\n",
    "            val_df = None\n",
    "            \n",
    "            if val_year != None:\n",
    "                class_df_pruned_rows = np.array([parse_time(x).datetime.year for x in class_df.event_starttime])\n",
    "                train_df = class_df[class_df_pruned_rows != val_year]\n",
    "                val_df = class_df[class_df_pruned_rows == val_year] \n",
    "            \n",
    "            p.apply_async(CreateBinaryAugmentedLSTMData, (save_dir, train_df, val_df, direction, starting_point, cadence, time_delta))\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1f0e038-779c-4246-971b-d66468092f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateResnetDataOfClass(class_df, test_split, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    test_split_num, val_split_num = math.ceil(event_num*test_split), math.ceil(event_num*val_split)\n",
    "    test_df = class_df[-test_split_num:]\n",
    "    class_df = class_df[:-test_split_num]\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -36\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(test_df, TEST_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(test_df, TEST_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(val_df, VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(val_df, VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(class_df, TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(class_df, TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    \n",
    "    # print(f'total training data: {len(train_df)*2}, total val data: {len(val_df)*2}, total test data: {len(test_df)*2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77401e82-5685-4736-aa92-fcd5cd56399d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateLSTMDataOfClass(class_df, test_split, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    test_split_num, val_split_num = math.ceil(event_num*test_split), math.ceil(event_num*val_split)\n",
    "    test_df = class_df[-test_split_num:]\n",
    "    class_df = class_df[:-test_split_num]\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = -6\n",
    "    random_mult = 6#np.random.randint(3, 7)\n",
    "    random_time_delta = random_mult * time_delta\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(test_df, LSTM_TRAIN_DATA_EXT_POSITIVE_DIR, time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(test_df, LSTM_TEST_DATA_EXT_NEGATIVE_DIR, random_time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(val_df, LSTM_VAL_DATA_EXT_POSITIVE_DIR, time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(val_df, LSTM_VAL_DATA_EXT_NEGATIVE_DIR, random_time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(class_df, LSTM_TRAIN_DATA_EXT_POSITIVE_DIR, time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(class_df, LSTM_TRAIN_DATA_EXT_NEGATIVE_DIR, random_time_delta, num_frames=20, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf4b0a11-7d85-4477-b76a-1b03885686ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateLSTMPairDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -36\n",
    "    random_mult = 6#np.random.randint(3, 7)\n",
    "    random_time_delta = random_mult * time_delta\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(class_df, LSTM_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=6, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(class_df, LSTM_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=6, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(val_df, LSTM_AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=6, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(val_df, LSTM_AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=6, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9aab2906-c8a4-4020-9d92-c4b9619ab513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreatePairDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -36\n",
    "    \n",
    "    SaveAIAPairFromDF(class_df, PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIAPairFromDF(class_df, PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIAPairFromDF(val_df, PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIAPairFromDF(val_df, PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4ff11ca-16a6-4099-831c-3f818d1bd0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateAugmentedDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -36\n",
    "    \n",
    "    SaveAIAAugmentedDataFromDF(class_df, AUG_PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=1, sample_mult=60, direction='forward', flare_class=classes)\n",
    "    SaveAIAAugmentedDataFromDF(class_df, AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, sample_mult=60, direction='backward', flare_class=classes)\n",
    "    \n",
    "    # SaveAIAAugmentedDataFromDF(val_df, AUG_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, sample_mult=60, direction='forward', flare_class=classes)\n",
    "    # SaveAIAAugmentedDataFromDF(val_df, AUG_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, sample_mult=60, direction='backward', flare_class=classes)\n",
    "    # SaveAIANCutoutsFromDF(val_df, AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    # SaveAIANCutoutsFromDF(val_df, AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    SaveAIAPairFromDF(val_df, AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIAPairFromDF(val_df, AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d54a268-0477-4ea9-820c-3c81e05e3ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = -18\n",
    "    random_time_delta = -60\n",
    "    num_frames = 6\n",
    "    \n",
    "    SaveAIANAugmentedDataFromDF(class_df, LSTM_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, positive=False, num_frames=num_frames, sample_mult=60, direction='forward', flare_class=classes)    \n",
    "    SaveAIANAugmentedDataFromDF(class_df, LSTM_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, positive=True, num_frames=num_frames, sample_mult=60, direction='forward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANAugmentedDataFromDF(val_df, LSTM_AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, positive=True, num_frames=num_frames, sample_mult=1, direction='forward', flare_class=classes)\n",
    "    SaveAIANAugmentedDataFromDF(val_df, LSTM_AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, positive=False, num_frames=num_frames, sample_mult=1, direction='forward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2526e7f-a0b3-461c-90ec-9c619c4ed087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateEndAugmentedLSTMDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 6\n",
    "    random_time_delta = -12#np.random.randint(0, 4)\n",
    "    \n",
    "    SaveAIANAugmentedDataFromDF(class_df, LSTM_END_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=6, sample_mult=60, direction='backward', flare_class=classes)\n",
    "    SaveAIANAugmentedDataFromDF(class_df, LSTM_END_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=6, sample_mult=60, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANAugmentedDataFromDF(val_df, LSTM_END_AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=6, sample_mult=1, direction='backward', flare_class=classes)\n",
    "    SaveAIANAugmentedDataFromDF(val_df, LSTM_END_AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=6, sample_mult=1, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65fe4e6c-e2dc-4163-9863-b42107761cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMDataAllClass(save_dir, train_df, val_df, direction, starting_point='event_starttime', cadence=6, time_delta=0, num_frames=6):\n",
    "    print('started CreateAugmentedLSTMDataAllClass')\n",
    "    train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    train_dir = os.path.join(save_dir, 'train')\n",
    "    val_dir = os.path.join(save_dir, 'val')\n",
    "   \n",
    "    SaveAIAAllClassAugmentedDataFromDF(train_df, train_dir, time_delta, num_frames=num_frames, sample_mult=None, direction=direction, starting_point=starting_point, cadence=cadence)   \n",
    "    SaveAIAAllClassAugmentedDataFromDF(val_df, val_dir, time_delta, num_frames=num_frames, sample_mult=1, direction=direction, starting_point=starting_point, cadence=cadence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47370dc2-400d-4da5-a47d-15ff53f79af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateBinaryAugmentedLSTMData(save_dir, train_df, val_split, direction, starting_point='event_starttime', cadence=6, time_delta=0):\n",
    "    print('started CreateBinaryAugmentedLSTMData')\n",
    "    train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    num_frames = 6\n",
    "    train_dir = os.path.join(save_dir, 'train')\n",
    "    val_dir = os.path.join(save_dir, 'val')\n",
    "    \n",
    "    SaveAIABinaryAugmentedDataFromDF(train_df, train_dir, time_delta, num_frames=num_frames, sample_mult=None, direction=direction, starting_point=starting_point, cadence=cadence)   \n",
    "    SaveAIABinaryAugmentedDataFromDF(val_df, val_dir, time_delta, num_frames=num_frames, sample_mult=1, direction=direction, starting_point=starting_point, cadence=cadence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee20f9c8-4cb8-4089-9ff8-95f5ca06085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.csv\n",
      "started CreateAugmentedLSTMDataAllClass\n",
      "started SaveAIAAllClassAugmentedDataFromDF\n",
      "H.csv\n",
      "started CreateAugmentedLSTMDataAllClass\n",
      "started SaveAIAAllClassAugmentedDataFromDF\n",
      "started CreateAugmentedLSTMDataAllClass\n",
      "started SaveAIAAllClassAugmentedDataFromDF\n",
      "X.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-21:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCreateAugmentedLSTMDataAll\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./new_data/ALL_lstm_data_hmx_during_leftout2013_cadence6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2013\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_starttime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcadence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36mCreateAugmentedLSTMDataAll\u001b[0;34m(save_dir, direction, val_year, starting_point, classes, cadence, time_delta, num_frames)\u001b[0m\n\u001b[1;32m     25\u001b[0m         p\u001b[38;5;241m.\u001b[39mapply_async(CreateAugmentedLSTMDataAllClass, (save_dir, train_df, val_df, direction, starting_point, cadence, time_delta, num_frames))\n\u001b[1;32m     26\u001b[0m p\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py:662\u001b[0m, in \u001b[0;36mPool.join\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (CLOSE, TERMINATE):\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn unknown state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 662\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_handler\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_handler\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/miniconda3/envs/mobilenet_v2_test/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mobilenet_v2_test/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/tmp/ipykernel_31940/1524072055.py\", line 8, in CreateAugmentedLSTMDataAllClass\n",
      "    SaveAIAAllClassAugmentedDataFromDF(train_df, train_dir, time_delta, num_frames=num_frames, sample_mult=None, direction=direction, starting_point=starting_point, cadence=cadence)\n",
      "  File \"/tmp/ipykernel_31940/3473704127.py\", line 42, in SaveAIAAllClassAugmentedDataFromDF\n",
      "    full_image_aug_data = np.array([GenerateRotateData(im, sample_mult, 512) for im in full_images])\n",
      "  File \"/tmp/ipykernel_31940/1524072055.py\", line 8, in CreateAugmentedLSTMDataAllClass\n",
      "    SaveAIAAllClassAugmentedDataFromDF(train_df, train_dir, time_delta, num_frames=num_frames, sample_mult=None, direction=direction, starting_point=starting_point, cadence=cadence)\n",
      "  File \"/tmp/ipykernel_31940/3473704127.py\", line 42, in <listcomp>\n",
      "    full_image_aug_data = np.array([GenerateRotateData(im, sample_mult, 512) for im in full_images])\n",
      "  File \"/tmp/ipykernel_31940/3473704127.py\", line 42, in SaveAIAAllClassAugmentedDataFromDF\n",
      "    full_image_aug_data = np.array([GenerateRotateData(im, sample_mult, 512) for im in full_images])\n",
      "  File \"/tmp2/nikitagalayda/flare_lstm/utils/data_augmentation.py\", line 122, in GenerateRotateData\n",
      "    return GetRotatedCutouts(im, cutout_size, rot_step)\n",
      "  File \"/tmp2/nikitagalayda/flare_lstm/utils/data_augmentation.py\", line 114, in GetRotatedCutouts\n",
      "    rotated_cutouts.append(GetRotatedCutout(im, angle, cutout_size))\n",
      "  File \"/tmp/ipykernel_31940/3473704127.py\", line 42, in <listcomp>\n",
      "    full_image_aug_data = np.array([GenerateRotateData(im, sample_mult, 512) for im in full_images])\n",
      "  File \"/tmp2/nikitagalayda/flare_lstm/utils/data_augmentation.py\", line 104, in GetRotatedCutout\n",
      "    rot_im = rotate(im, angle)\n",
      "  File \"/tmp2/nikitagalayda/flare_lstm/utils/data_augmentation.py\", line 122, in GenerateRotateData\n",
      "    return GetRotatedCutouts(im, cutout_size, rot_step)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/site-packages/scipy/ndimage/_interpolation.py\", line 940, in rotate\n",
      "    affine_transform(input_arr, rot_matrix, offset, output_shape, output,\n",
      "  File \"/tmp2/nikitagalayda/flare_lstm/utils/data_augmentation.py\", line 114, in GetRotatedCutouts\n",
      "    rotated_cutouts.append(GetRotatedCutout(im, angle, cutout_size))\n",
      "  File \"/tmp2/nikitagalayda/flare_lstm/utils/data_augmentation.py\", line 104, in GetRotatedCutout\n",
      "    rot_im = rotate(im, angle)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/site-packages/scipy/ndimage/_interpolation.py\", line 611, in affine_transform\n",
      "    _nd_image.geometric_transform(filtered, None, None, matrix, offset,\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/site-packages/scipy/ndimage/_interpolation.py\", line 940, in rotate\n",
      "    affine_transform(input_arr, rot_matrix, offset, output_shape, output,\n",
      "KeyboardInterrupt\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/site-packages/scipy/ndimage/_interpolation.py\", line 611, in affine_transform\n",
      "    _nd_image.geometric_transform(filtered, None, None, matrix, offset,\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/tmp/ipykernel_31940/1524072055.py\", line 8, in CreateAugmentedLSTMDataAllClass\n",
      "    SaveAIAAllClassAugmentedDataFromDF(train_df, train_dir, time_delta, num_frames=num_frames, sample_mult=None, direction=direction, starting_point=starting_point, cadence=cadence)\n",
      "  File \"/tmp/ipykernel_31940/3473704127.py\", line 40, in SaveAIAAllClassAugmentedDataFromDF\n",
      "    aug_data = GenerateAugmentedLSTMData(paths, coord, sample_mult)\n",
      "  File \"/tmp/ipykernel_31940/957067039.py\", line 2, in GenerateAugmentedLSTMData\n",
      "    full_images = [np.load(im)['x'] for im in paths]\n",
      "  File \"/tmp/ipykernel_31940/957067039.py\", line 2, in <listcomp>\n",
      "    full_images = [np.load(im)['x'] for im in paths]\n",
      "  File \"/home/master/09/nikitagalayda/miniconda3/envs/mobilenet_v2_test/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 414, in load\n",
      "    magic = fid.read(N)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "CreateAugmentedLSTMDataAll('./new_data/ALL_lstm_data_hmx_during_leftout2013_cadence6', 'forward', 2013, 'event_starttime', ['H', 'M', 'X'], cadence=6, time_delta=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812de66-b86f-4f37-a246-2b756488d34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
