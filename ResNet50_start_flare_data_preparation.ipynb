{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bca0536-9b58-4bdf-8324-f4e73528d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sunpy.map\n",
    "from sunpy.time import parse_time\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "import astropy.stats\n",
    "import math\n",
    "import os\n",
    "import threading\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "import sys\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import random\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "sys.path.append(os.path.join(Path.cwd(), 'utils'))\n",
    "\n",
    "from utils.im_utils import *\n",
    "from utils.data_augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feb98fe-b71a-4f0e-8262-1192c1ec4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa998fa6-ff3a-4739-af71-936e8b559035",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLARE_CLASS = 'ALL'\n",
    "\n",
    "AIA_DATA_DIR = '../data'\n",
    "EVENTS_BY_DATE_DIR = './event_records/new_events_by_date'\n",
    "EVENTS_BY_CLASS_DIR = './event_records/new_events_by_class'\n",
    "\n",
    "TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data/train/positive'\n",
    "TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data/train/negative'\n",
    "TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data/test/positive'\n",
    "TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data/test/negative'\n",
    "VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data/val/positive'\n",
    "VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data/val/negative'\n",
    "\n",
    "AUG_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/train/positive'\n",
    "AUG_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/train/negative'\n",
    "AUG_TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/test/positive'\n",
    "AUG_TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/test/negative'\n",
    "AUG_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/val/positive'\n",
    "AUG_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented/val/negative'\n",
    "\n",
    "AUG_PAIR_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/train/positive'\n",
    "AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/train/negative'\n",
    "AUG_PAIR_TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/test/positive'\n",
    "AUG_PAIR_TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/test/negative'\n",
    "AUG_PAIR_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/val/positive'\n",
    "AUG_PAIR_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_data_augmented_pair/val/negative'\n",
    "\n",
    "LSTM_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/train/positive'\n",
    "LSTM_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/train/negative'\n",
    "LSTM_TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/test/positive'\n",
    "LSTM_TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/test/negative'\n",
    "LSTM_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/val/positive'\n",
    "LSTM_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data/val/negative'\n",
    "\n",
    "LSTM_TRAIN_DATA_EXT_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/train/positive'\n",
    "LSTM_TRAIN_DATA_EXT_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/train/negative'\n",
    "LSTM_TEST_DATA_EXT_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/test/positive'\n",
    "LSTM_TEST_DATA_EXT_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/test/negative'\n",
    "LSTM_VAL_DATA_EXT_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/val/positive'\n",
    "LSTM_VAL_DATA_EXT_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_extended/val/negative'\n",
    "\n",
    "LSTM_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/train/positive'\n",
    "LSTM_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/train/negative'\n",
    "LSTM_AUG_PAIR_TEST_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/test/positive'\n",
    "LSTM_AUG_PAIR_TEST_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/test/negative'\n",
    "LSTM_AUG_PAIR_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/val/positive'\n",
    "LSTM_AUG_PAIR_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_pair/val/negative'\n",
    "\n",
    "LSTM_AUG_ALL_CLASS_TRAIN_DATA_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented/train/'\n",
    "LSTM_AUG_ALL_CLASS_VAL_DATA_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented/val/'\n",
    "\n",
    "LSTM_AUG_ALL_CLASS_PRIOR_TRAIN_DATA_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_prior/train/'\n",
    "LSTM_AUG_ALL_CLASS_PRIOR_VAL_DATA_DIR = f'./data/{FLARE_CLASS}_lstm_data_augmented_prior/val/'\n",
    "\n",
    "LSTM_END_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_end_data_augmented_pair/train/positive'\n",
    "LSTM_END_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_end_data_augmented_pair/train/negative'\n",
    "LSTM_END_AUG_PAIR_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_lstm_end_data_augmented_pair/val/positive'\n",
    "LSTM_END_AUG_PAIR_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_lstm_end_data_augmented_pair/val/negative'\n",
    "\n",
    "PAIR_TRAIN_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_full_image_data/train/positive'\n",
    "PAIR_TRAIN_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_full_image_data/train/negative'\n",
    "PAIR_VAL_DATA_POSITIVE_DIR = f'./data/{FLARE_CLASS}_full_image_data/val/positive'\n",
    "PAIR_VAL_DATA_NEGATIVE_DIR = f'./data/{FLARE_CLASS}_full_image_data/val/negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a8dba5-c70d-4ba6-8360-e714efacdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDELT = 0.599733;\n",
    "HPCCENTER = 4096.0 / 2.0;\n",
    "rsun_meters = 696000;\n",
    "dsun_meters = 149600000;\n",
    "DEFAULT_WIDTH, DEFAULT_HEIGHT = 64, 64\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 512, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec4f53d-afbe-4345-8528-225b5c1ec0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614945d0-242f-4dee-9d5e-ba76fdda240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_data():\n",
    "    delete_files(TRAIN_DATA_POSITIVE_DIR)\n",
    "    delete_files(TRAIN_DATA_NEGATIVE_DIR)\n",
    "    delete_files(TEST_DATA_POSITIVE_DIR)\n",
    "    delete_files(TEST_DATA_NEGATIVE_DIR)\n",
    "    delete_files(VAL_DATA_POSITIVE_DIR)\n",
    "    delete_files(VAL_DATA_NEGATIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9497b771-1008-4ad0-ab9f-830e52d74ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lstm_data():\n",
    "    delete_files(LSTM_TRAIN_DATA_POSITIVE_DIR)\n",
    "    delete_files(LSTM_TRAIN_DATA_NEGATIVE_DIR)\n",
    "    delete_files(LSTM_TEST_DATA_POSITIVE_DIR)\n",
    "    delete_files(LSTM_TEST_DATA_NEGATIVE_DIR)\n",
    "    delete_files(LSTM_VAL_DATA_POSITIVE_DIR)\n",
    "    delete_files(LSTM_VAL_DATA_NEGATIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35cfd6f0-cc87-4a44-a3e4-d3dab74db286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pair_data():\n",
    "    delete_files(PAIR_TRAIN_DATA_POSITIVE_DIR)\n",
    "    delete_files(PAIR_TRAIN_DATA_NEGATIVE_DIR)\n",
    "    delete_files(PAIR_VAL_DATA_POSITIVE_DIR)\n",
    "    delete_files(PAIR_VAL_DATA_NEGATIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb4fb487-8562-46a9-a3cc-3d3882e10400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_augmented_data():\n",
    "    delete_files(AUG_TRAIN_DATA_POSITIVE_DIR)\n",
    "    delete_files(AUG_TRAIN_DATA_NEGATIVE_DIR)\n",
    "    delete_files(AUG_TEST_DATA_POSITIVE_DIR)\n",
    "    delete_files(AUG_TEST_DATA_NEGATIVE_DIR)\n",
    "    delete_files(AUG_VAL_DATA_POSITIVE_DIR)\n",
    "    delete_files(AUG_VAL_DATA_NEGATIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c61e2e6a-c6c9-4af2-86df-13e13b61d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_augmented_pair_data():\n",
    "    delete_files(AUG_PAIR_TRAIN_DATA_POSITIVE_DIR)\n",
    "    delete_files(AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR)\n",
    "    delete_files(AUG_PAIR_TEST_DATA_POSITIVE_DIR)\n",
    "    delete_files(AUG_PAIR_TEST_DATA_NEGATIVE_DIR)\n",
    "    delete_files(AUG_PAIR_VAL_DATA_POSITIVE_DIR)\n",
    "    delete_files(AUG_PAIR_VAL_DATA_NEGATIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9447a448-30f6-43e4-b0da-9679b29d4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lstm_augmented_pair_data():\n",
    "    delete_files(LSTM_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR)\n",
    "    delete_files(LSTM_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR)\n",
    "    delete_files(LSTM_AUG_PAIR_VAL_DATA_POSITIVE_DIR)\n",
    "    delete_files(LSTM_AUG_PAIR_VAL_DATA_NEGATIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "295f09c3-b51c-4c3e-89f9-1e56c0345fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lstm_end_augmented_pair_data():\n",
    "    delete_files(LSTM_END_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR)\n",
    "    delete_files(LSTM_END_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR)\n",
    "    delete_files(LSTM_END_AUG_PAIR_VAL_DATA_POSITIVE_DIR)\n",
    "    delete_files(LSTM_END_AUG_PAIR_VAL_DATA_NEGATIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e21c0ce-f00e-46c1-aa8c-1b0df00f5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lstm_augmented_all_data():\n",
    "    delete_files(LSTM_AUG_ALL_CLASS_PRIOR_TRAIN_DATA_DIR)\n",
    "    delete_files(LSTM_AUG_ALL_CLASS_PRIOR_VAL_DATA_DIR)\n",
    "    delete_files(f'{LSTM_AUG_ALL_CLASS_PRIOR_TRAIN_DATA_DIR}/N')\n",
    "    delete_files(f'{LSTM_AUG_ALL_CLASS_PRIOR_VAL_DATA_DIR}/N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7a971e-3b4f-42cd-8052-ffff85ce4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateAugmentedLSTMData(paths, coord, sample_mult):\n",
    "    full_images = [np.load(im)['x'] for im in paths]\n",
    "    large_cutouts = []\n",
    "    for full_im in full_images:\n",
    "        large_cutouts.append(GetSafeCenteredCutout(full_im, 128, coord))\n",
    "        \n",
    "    rot_data = np.array([GenerateRotateData(im, sample_mult, 64) for im in large_cutouts])\n",
    "    # print(rot_data.shape)\n",
    "    rot_data_arranged = []\n",
    "    for i in range(rot_data.shape[0]):\n",
    "        rot_data_arranged.append([rot_data[i][x] for x in range(rot_data.shape[1])])\n",
    "    rot_data_arranged = np.array(rot_data_arranged)\n",
    "    # flip_data = np.array([GenerateFlipData(im, cutout_size=64) for im in large_cutouts])\n",
    "    # print(flip_data.shape)\n",
    "    # zoom_data = np.array([GenerateZoomData(im, cutout_size=64) for im in large_cutouts])\n",
    "    # print(zoom_data.shape)\n",
    "    \n",
    "    return rot_data_arranged#np.concatenate((rot_data, flip_data, zoom_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53c4a33e-71a6-4dd3-af00-dc59d8a5b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    '../data/2016/01/01/AIA20160101_0000_0094.npz',\n",
    "    '../data/2016/01/01/AIA20160101_0006_0094.npz',\n",
    "    '../data/2016/01/01/AIA20160101_0012_0094.npz',\n",
    "    '../data/2016/01/01/AIA20160101_0018_0094.npz',\n",
    "    '../data/2016/01/01/AIA20160101_0024_0094.npz',\n",
    "    '../data/2016/01/01/AIA20160101_0030_0094.npz',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "576afe88-7c5a-4860-a7e0-6cc47bd0f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = GenerateAugmentedLSTMData(test_data, (20, 100), 60)\n",
    "test = np.reshape(test, (60, 6, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84860f29-f42d-45d1-b64f-4b6ea566df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "num = 0\n",
    "for subdir, dirs, files in os.walk(f'./data/M_lstm_data_augmented_pair/train/positive/AIA20111031_1721_0094/20/sequence'):\n",
    "    for f in files:\n",
    "        data.append(os.path.join(subdir, f))\n",
    "full_data = []\n",
    "for subdir, dirs, files in os.walk(f'./data/M_lstm_data_augmented_pair/train/positive/AIA20111031_1721_0094/0/full'):\n",
    "    for f in files:\n",
    "        full_data.append(os.path.join(subdir, f))\n",
    "data = [np.load(x) for x in data]\n",
    "full_data = [np.load(x) for x in full_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2679576e-00bd-41c0-b166-c02d84c79121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAIAFormatFilename(dt):\n",
    "    AIA_data_date = f'{dt.year}{dt.month:02d}{dt.day:02d}'\n",
    "    AIA_data_time = f'{dt.hour:02d}{dt.minute:02d}'\n",
    "    AIA_data_filename = f'AIA{AIA_data_date}_{AIA_data_time}_0094'\n",
    "    \n",
    "    return AIA_data_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a3f3284-f780-4ee3-910d-3ea5d527eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a formatted file name of the closest AIA data file to the given datetime\n",
    "\n",
    "def GetClosestDataFileByDate(dt, rounding):\n",
    "    AIA_data_date = f'{dt.year}{dt.month:02d}{dt.day:02d}'\n",
    "    tmp_dt = dt\n",
    "    minute = 0\n",
    "    minute=GetClosestMultiple(tmp_dt.minute, 6)\n",
    "    AIA_data_time = f'{tmp_dt.hour:02d}{minute:02d}'\n",
    "    AIA_data_filename = f'AIA{AIA_data_date}_{AIA_data_time}_0094.npz'\n",
    "    \n",
    "    return AIA_data_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b40db44-68bd-452f-9e7b-43c97ccd768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCutout(im, coord, N=64):\n",
    "    x_end_idx = int(coord[0]+N)\n",
    "    y_end_idx = int(coord[1]+N)\n",
    "    cutout_array = im[int(coord[0]):x_end_idx, int(coord[1]):y_end_idx]\n",
    "    \n",
    "    return cutout_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc4213e6-922b-4710-b692-eb96b2e8d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCenteredCutout(im, coord, N=64):\n",
    "    x, y = int(coord[0]), int(coord[1])\n",
    "    offset = N//2\n",
    "    x_start, x_end = x-offset, x+offset\n",
    "    y_start, y_end = y-offset, y+offset\n",
    "    cutout_array = im[x_start:x_end, y_start:y_end]\n",
    "    \n",
    "    return cutout_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fed19cb9-2a54-4041-9d38-d6f33624bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an NxN cutout of a file centered around the given coordinate\n",
    "\n",
    "def GetFileCutout(path, coord, N=64):\n",
    "    im = np.load(path)['x']\n",
    "    # scs = astropy.stats.sigma_clipped_stats(im)\n",
    "    # mean, median, sd = scs[0], scs[1], scs[2]\n",
    "    # thr = median+sd*3\n",
    "    # im[im<thr]=0\n",
    "    \n",
    "    return GetCutout(im, coord, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4529c69-de72-4dba-b80a-c9d091e16e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an AIA filepath closest to the provided\n",
    "\n",
    "def GetAIAPathAtTime(dt):\n",
    "    dt_data_dir = os.path.join(AIA_DATA_DIR, f'{dt.year}/{dt.month:02d}/{dt.day:02d}')\n",
    "    closest_data_file = GetClosestDataFileByDate(dt, \"up\")\n",
    "    file_path = os.path.join(dt_data_dir, closest_data_file)\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc2173d-c22d-47a2-b272-a678598cd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedCutouts(coord, path, save_path, rotation_step=12):\n",
    "    # create augmented cutouts which are rotated, flipped, and mirrored\n",
    "    cutout =  GetFileCutout(path, coord, N=128)\n",
    "    return cutout\n",
    "    cur_rot = rotation_step\n",
    "    for i in range(360//rotation_step):\n",
    "        rot_image = rotate(cutout, 0)\n",
    "        rot_cutout = GetCenteredCutout(rot_image, (rot_image.shape[0]//2, rot_image.shape[1]//2), 64)\n",
    "        cur_rot += rotation_step\n",
    "        return rot_cutout\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0847ac90-0c55-4bf7-933a-071152537177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# returns M consecutive (forward or backward) AIA filepaths starting at time closest to the given time\n",
    "\n",
    "def GetAIANCutoutsPaths(start_dt, direction='backward', M=6, cadence=6):\n",
    "    cutouts_paths = []\n",
    "    if direction == 'backward':\n",
    "        cadence = -cadence\n",
    "    dynamic_dt = start_dt\n",
    "        \n",
    "    for i in range(M):\n",
    "        try:\n",
    "            cutout_path = GetAIAPathAtTime(dynamic_dt)\n",
    "        except FileNotFoundError:\n",
    "            dynamic_dt = dynamic_dt + datetime.timedelta(minutes=cadence)\n",
    "            continue\n",
    "        cutouts_paths.append(cutout_path)\n",
    "        dynamic_dt = dynamic_dt + datetime.timedelta(minutes=cadence)\n",
    "    \n",
    "    cutouts_paths = sorted(cutouts_paths)\n",
    "    num_paths = len(cutouts_paths)\n",
    "    filled_paths = cutouts_paths\n",
    "\n",
    "    if num_paths != M:\n",
    "        # print(f'filling files, start_dt: {start_dt} num_paths: {num_paths} len filled_paths: {len(filled_paths)}')\n",
    "        diff = abs(M-num_paths)\n",
    "        # print(f'diff: {diff} num-diff: {num_paths-diff}')\n",
    "        for i in range(num_paths, num_paths-diff, -1):\n",
    "            filled_paths.insert(i-1, filled_paths[i-1])\n",
    "            # print(f'inserted {filled_paths[i-1]} at pos {i-1}')\n",
    "        filled_paths = sorted(filled_paths)\n",
    "        # print(f'after filling: {np.array(filled_paths)}')\n",
    "    filled_paths = sorted(filled_paths)\n",
    "    return np.array(filled_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2929b5a-f5c4-4eab-b2a8-0198df8947a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIANCutoutsFromDF(df, save_dir, time_delta, num_frames=1, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0]-32, 512-coord[1]-32)\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "        \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "        \n",
    "        if len(paths) > 1:\n",
    "            folder_name = GetAIAFormatFilename(start_dt)\n",
    "            event_folder_path = os.path.join(save_dir, folder_name)\n",
    "            if not os.path.exists(event_folder_path):\n",
    "                os.makedirs(event_folder_path)\n",
    "            else:\n",
    "                print(f'trying to create existing folder, time: {start_dt}')\n",
    "                continue \n",
    "            mod_save_dir = event_folder_path\n",
    "        for idx, p in enumerate(paths):\n",
    "            cutout = GetFileCutout(p, coord)\n",
    "            save_filename = os.path.basename(p).rsplit('.', 1)[0]\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_{idx}', cutout)\n",
    "    # print(f'{nonexistent} files did not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68d6890a-6c8e-4505-a38d-eb008d6b7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIAPairFromDF(df, save_dir, time_delta, num_frames=1, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0]-32, 512-coord[1]-32)\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        # full sun image path\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "        \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "            \n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(save_dir, folder_name)\n",
    "        if not os.path.exists(event_folder_path):\n",
    "            os.makedirs(event_folder_path)\n",
    "        else:\n",
    "            print(f'trying to create existing folder, time: {start_dt}')\n",
    "            continue \n",
    "        mod_save_dir = event_folder_path\n",
    "        for idx, p in enumerate(paths):\n",
    "            cutout = GetFileCutout(p, coord)\n",
    "            save_filename = os.path.basename(p).rsplit('.', 1)[0]\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_{idx}', cutout)\n",
    "            \n",
    "            full_image = np.load(closest_file_path)['x']\n",
    "            full_image = cv2.resize(full_image, (128, 128), interpolation = cv2.INTER_AREA)\n",
    "            scs = astropy.stats.sigma_clipped_stats(full_image)\n",
    "            mean, median, sd = scs[0], scs[1], scs[2]\n",
    "            thr = median+sd*3\n",
    "            full_image[full_image<thr]=0\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_full', full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2527d7c-b7e6-4385-af98-3c3d63b76aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIAPairFromDF(df, save_dir, time_delta, num_frames=1, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0]-32, 512-coord[1]-32)\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        # full sun image path\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "        \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "            \n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(save_dir, folder_name)\n",
    "        if not os.path.exists(event_folder_path):\n",
    "            os.makedirs(event_folder_path)\n",
    "        else:\n",
    "            print(f'trying to create existing folder, time: {start_dt}')\n",
    "            continue \n",
    "        mod_save_dir = event_folder_path\n",
    "        for idx, p in enumerate(paths):\n",
    "            cutout = GetFileCutout(p, coord)\n",
    "            save_filename = os.path.basename(p).rsplit('.', 1)[0]\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_{idx}', cutout)\n",
    "            \n",
    "            full_image = np.load(closest_file_path)['x']\n",
    "            full_image = cv2.resize(full_image, (128, 128), interpolation = cv2.INTER_AREA)\n",
    "            scs = astropy.stats.sigma_clipped_stats(full_image)\n",
    "            mean, median, sd = scs[0], scs[1], scs[2]\n",
    "            thr = median+sd*3\n",
    "            full_image[full_image<thr]=0\n",
    "            np.save(f'{mod_save_dir}/{save_filename}_full', full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af2a239f-4a6c-4ef2-8a62-634b4988c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveAIAAugmentedDataFromDF(df, save_dir, time_delta, num_frames=1, sample_mult=60, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0], 512-coord[1])\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "        \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "        \n",
    "        for idx, p in enumerate(paths):\n",
    "            full_image = np.load(closest_file_path)['x']\n",
    "            aug_data = GenerateAugmentedData(full_image, coord, sample_mult)\n",
    "            aug_data = np.concatenate((aug_data))\n",
    "            for idx, d in enumerate(aug_data):\n",
    "                folder_name = GetAIAFormatFilename(start_dt)\n",
    "                event_folder_path = f'{os.path.join(save_dir, folder_name)}_{idx}'\n",
    "                if not os.path.exists(event_folder_path):\n",
    "                    os.makedirs(event_folder_path)\n",
    "                else:\n",
    "                    print(f'trying to create existing folder, time: {start_dt}')\n",
    "                    continue \n",
    "                mod_save_dir = event_folder_path\n",
    "                save_filename = os.path.basename(p).rsplit('.', 1)[0]\n",
    "                np.save(f'{mod_save_dir}/{save_filename}_augmented_{idx}', d)\n",
    "                np.save(f'{mod_save_dir}/{save_filename}_full', full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d0e50dc-5943-44b2-8ff6-3002a14872d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIANAugmentedDataFromDF(df, save_dir, time_delta, positive=True, num_frames=1, sample_mult=60, direction='backward', flare_class='all'):\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        # if not positive:\n",
    "        #     time_delta = -18\n",
    "            # time_delta = random.choice([-36, 12])\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if flare_class != 'all':\n",
    "            if goes_cls != flare_class:\n",
    "                continue\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0], 512-coord[1])\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "            \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        mod_save_dir = save_dir\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "        \n",
    "        paths = sorted(paths)\n",
    "            \n",
    "        aug_data = GenerateAugmentedLSTMData(paths, coord, sample_mult)\n",
    "\n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(save_dir, folder_name)\n",
    "        for i in range(aug_data.shape[1]):\n",
    "            subfolder_path = os.path.join(event_folder_path, str(i))\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.makedirs(subfolder_path)\n",
    "            sequence_folder_path = os.path.join(subfolder_path, 'sequence')\n",
    "            if not os.path.exists(sequence_folder_path):\n",
    "                os.makedirs(sequence_folder_path)\n",
    "            full_images_folder_path = os.path.join(subfolder_path, 'full')\n",
    "            if not os.path.exists(full_images_folder_path):\n",
    "                os.makedirs(full_images_folder_path)\n",
    "            for j in range(aug_data.shape[0]):\n",
    "                aia_filename = paths[j].rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "                save_filename = f'{aia_filename}_{j}'\n",
    "                np.save(f'{sequence_folder_path}/{save_filename}', aug_data[j][i])\n",
    "                full_image_save_filename = f'{folder_name}_{j}'\n",
    "                np.save(f'{full_images_folder_path}/{save_filename}', np.load(paths[j])['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "828653a2-38a4-4876-83ff-a301b2f3a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### saves directional sequences of NxN cutouts of all events in the csv file to folders with start times as names\n",
    "\n",
    "def SaveAIAAllClassAugmentedDataFromDF(df, save_dir, time_delta, num_frames=1, sample_mult=None, direction='backward', flare_class='all'):\n",
    "    # C: 6127 M: 612 X: 38\n",
    "    # multiplied by 2, 20, 320\n",
    "    # C: 12,254 M: 12,240 X: 12,160\n",
    "    print('started SaveAIAAllClassAugmentedDataFromDF')\n",
    "    nonexistent = 0\n",
    "    for index, row in df.iterrows():\n",
    "        raw_time, goes_cls = parse_time(row['event_starttime'], precision=0), row['fl_goescls'][0]\n",
    "        if goes_cls != 'C' and goes_cls != 'M' and goes_cls != 'X':\n",
    "            continue\n",
    "        cur_save_dir = os.path.join(save_dir, goes_cls)\n",
    "        start_dt, y, x = raw_time.datetime, int(row['hpc_x']), int(row['hpc_y'])\n",
    "        prior_dt = start_dt + datetime.timedelta(minutes=time_delta)\n",
    "        coord = ConvertHPCToPixXY((x, y))\n",
    "        coord = ResizeCoord(coord)\n",
    "        coord = (coord[0], 512-coord[1])\n",
    "        \n",
    "        closest_file_name = GetClosestDataFileByDate(prior_dt, rounding='up')\n",
    "        closest_file_path = f'{AIA_DATA_DIR}/{prior_dt.year}/{prior_dt.month:02}/{prior_dt.day:02}/{closest_file_name}'\n",
    "\n",
    "        if not os.path.exists(closest_file_path):\n",
    "            nonexistent+=1\n",
    "            continue\n",
    "            \n",
    "        paths = GetAIANCutoutsPaths(prior_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) != num_frames:\n",
    "            print(start_dt)\n",
    "            print(len(paths))\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "            \n",
    "        N_dt = start_dt + datetime.timedelta(minutes=-60)\n",
    "        N_closest_file_name = GetClosestDataFileByDate(N_dt, rounding='up')\n",
    "        N_closest_file_path = f'{AIA_DATA_DIR}/{N_dt.year}/{N_dt.month:02}/{N_dt.day:02}/{N_closest_file_name}'\n",
    "        if not os.path.exists(N_closest_file_path):\n",
    "            continue   \n",
    "        N_paths = GetAIANCutoutsPaths(N_dt, direction=direction, M=num_frames, cadence=6)\n",
    "        if len(paths) == 0:\n",
    "            print('0 paths')\n",
    "            continue\n",
    "        \n",
    "        paths = sorted(paths)\n",
    "        N_paths = sorted(N_paths)\n",
    "        N_aug_data = None\n",
    "        \n",
    "        if sample_mult == None:\n",
    "            if goes_cls == 'C':\n",
    "                sample_mult = 2\n",
    "            elif goes_cls == 'M':\n",
    "                sample_mult = 20\n",
    "            elif goes_cls == 'X':\n",
    "                sample_mult = 320\n",
    "        aug_data = GenerateAugmentedLSTMData(paths, coord, sample_mult)\n",
    "\n",
    "        folder_name = GetAIAFormatFilename(start_dt)\n",
    "        event_folder_path = os.path.join(cur_save_dir, folder_name)\n",
    "        for i in range(aug_data.shape[1]):\n",
    "            subfolder_path = os.path.join(event_folder_path, str(i))\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.makedirs(subfolder_path)\n",
    "            sequence_folder_path = os.path.join(subfolder_path, 'sequence')\n",
    "            if not os.path.exists(sequence_folder_path):\n",
    "                os.makedirs(sequence_folder_path)\n",
    "            full_images_folder_path = os.path.join(subfolder_path, 'full')\n",
    "            if not os.path.exists(full_images_folder_path):\n",
    "                os.makedirs(full_images_folder_path)\n",
    "            for j in range(aug_data.shape[0]):\n",
    "                aia_filename = paths[j].rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "                save_filename = f'{aia_filename}_{j}'\n",
    "                np.save(f'{sequence_folder_path}/{save_filename}', aug_data[j][i])\n",
    "                full_image_save_filename = f'{folder_name}_{j}'\n",
    "                np.save(f'{full_images_folder_path}/{save_filename}', np.load(paths[j])['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed42af3f-a704-480a-af7c-6c4170a7ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateResnetData(test_split=0.25, val_split=0.15, classes='all'):\n",
    "    delete_data()\n",
    "    train_split = 1-test_split+val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateResnetDataOfClass(class_df, test_split, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ad92ff2-1778-42af-8bc5-b6deda2545e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLSTMData(val_split=0.3, classes='all'):\n",
    "    delete_lstm_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateLSTMDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c697257f-406e-42a2-be37-daa5e36c7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLSTMPairData(val_split=0.3, classes='all'):\n",
    "    delete_lstm_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateLSTMPairDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e087239-d2c1-4ba7-9630-8468ce480624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreatePairData(val_split=0.3, classes='all'):\n",
    "    delete_pair_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreatePairDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0bbd696-14c1-4712-8ee2-4fd092f91aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedData(val_split=0.3, classes='all'):\n",
    "    # delete_augmented_data()\n",
    "    delete_augmented_pair_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateAugmentedDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "422dd4b4-9b74-4242-9c37-632fa005cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMData(val_split=0.3, classes='all'):\n",
    "    # delete_augmented_data()\n",
    "    delete_lstm_augmented_pair_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateAugmentedLSTMDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "806e917b-fb6f-48ad-bf5a-9c11fdf15a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateEndAugmentedLSTMData(val_split=0.3, classes='all'):\n",
    "    delete_lstm_end_augmented_pair_data()\n",
    "    train_split = 1-val_split\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if classes != 'all':\n",
    "                if file.rsplit('.', 1)[0] != classes:\n",
    "                    continue\n",
    "            print(file.rsplit('.', 1)[0])\n",
    "            class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "            CreateEndAugmentedLSTMDataOfClass(class_df, val_split, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcb9ad15-1877-4f37-b2ba-210df99ac74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMDataAll(val_split=0.3, classes='all'):\n",
    "    delete_lstm_augmented_all_data()\n",
    "    train_split = 1-val_split\n",
    "    # p = Pool(8)\n",
    "    for subdir, dirs, files in os.walk(EVENTS_BY_CLASS_DIR):\n",
    "        for file in files:\n",
    "            if file == 'M.csv' or file == 'X.csv':\n",
    "                print(file.rsplit('.', 1)[0])\n",
    "                class_df = pd.read_csv(os.path.join(subdir, file))\n",
    "                # p.apply_async(CreateAugmentedLSTMDataAllClass, (class_df, val_split, classes))\n",
    "                CreateAugmentedLSTMDataAllClass(class_df, val_split, classes)\n",
    "    # p.close()\n",
    "    # p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1f0e038-779c-4246-971b-d66468092f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateResnetDataOfClass(class_df, test_split, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    test_split_num, val_split_num = math.ceil(event_num*test_split), math.ceil(event_num*val_split)\n",
    "    test_df = class_df[-test_split_num:]\n",
    "    class_df = class_df[:-test_split_num]\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -36\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(test_df, TEST_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(test_df, TEST_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(val_df, VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(val_df, VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(class_df, TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(class_df, TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    \n",
    "    # print(f'total training data: {len(train_df)*2}, total val data: {len(val_df)*2}, total test data: {len(test_df)*2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77401e82-5685-4736-aa92-fcd5cd56399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLSTMDataOfClass(class_df, test_split, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    test_split_num, val_split_num = math.ceil(event_num*test_split), math.ceil(event_num*val_split)\n",
    "    test_df = class_df[-test_split_num:]\n",
    "    class_df = class_df[:-test_split_num]\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = -6\n",
    "    random_mult = 6#np.random.randint(3, 7)\n",
    "    random_time_delta = random_mult * time_delta\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(test_df, LSTM_TRAIN_DATA_EXT_POSITIVE_DIR, time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(test_df, LSTM_TEST_DATA_EXT_NEGATIVE_DIR, random_time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(val_df, LSTM_VAL_DATA_EXT_POSITIVE_DIR, time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(val_df, LSTM_VAL_DATA_EXT_NEGATIVE_DIR, random_time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(class_df, LSTM_TRAIN_DATA_EXT_POSITIVE_DIR, time_delta, num_frames=20, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(class_df, LSTM_TRAIN_DATA_EXT_NEGATIVE_DIR, random_time_delta, num_frames=20, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf4b0a11-7d85-4477-b76a-1b03885686ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLSTMPairDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -36\n",
    "    random_mult = 6#np.random.randint(3, 7)\n",
    "    random_time_delta = random_mult * time_delta\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(class_df, LSTM_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=6, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(class_df, LSTM_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=6, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANCutoutsFromDF(val_df, LSTM_AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=6, direction='backward', flare_class=classes)\n",
    "    SaveAIANCutoutsFromDF(val_df, LSTM_AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=6, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9aab2906-c8a4-4020-9d92-c4b9619ab513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreatePairDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -36\n",
    "    \n",
    "    SaveAIAPairFromDF(class_df, PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIAPairFromDF(class_df, PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIAPairFromDF(val_df, PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIAPairFromDF(val_df, PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4ff11ca-16a6-4099-831c-3f818d1bd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -36\n",
    "    \n",
    "    SaveAIAAugmentedDataFromDF(class_df, AUG_PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=1, sample_mult=60, direction='forward', flare_class=classes)\n",
    "    SaveAIAAugmentedDataFromDF(class_df, AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, sample_mult=60, direction='backward', flare_class=classes)\n",
    "    \n",
    "    # SaveAIAAugmentedDataFromDF(val_df, AUG_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, sample_mult=60, direction='forward', flare_class=classes)\n",
    "    # SaveAIAAugmentedDataFromDF(val_df, AUG_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, sample_mult=60, direction='backward', flare_class=classes)\n",
    "    # SaveAIANCutoutsFromDF(val_df, AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    # SaveAIANCutoutsFromDF(val_df, AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)\n",
    "    SaveAIAPairFromDF(val_df, AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=1, direction='forward', flare_class=classes)\n",
    "    SaveAIAPairFromDF(val_df, AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=1, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d54a268-0477-4ea9-820c-3c81e05e3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = -18\n",
    "    random_time_delta = -60\n",
    "    num_frames = 6\n",
    "    \n",
    "    SaveAIANAugmentedDataFromDF(class_df, LSTM_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, positive=False, num_frames=num_frames, sample_mult=60, direction='forward', flare_class=classes)    \n",
    "    SaveAIANAugmentedDataFromDF(class_df, LSTM_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, positive=True, num_frames=num_frames, sample_mult=60, direction='forward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANAugmentedDataFromDF(val_df, LSTM_AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, positive=True, num_frames=num_frames, sample_mult=1, direction='forward', flare_class=classes)\n",
    "    SaveAIANAugmentedDataFromDF(val_df, LSTM_AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, positive=False, num_frames=num_frames, sample_mult=1, direction='forward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2526e7f-a0b3-461c-90ec-9c619c4ed087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateEndAugmentedLSTMDataOfClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 6\n",
    "    random_time_delta = -12#np.random.randint(0, 4)\n",
    "    \n",
    "    SaveAIANAugmentedDataFromDF(class_df, LSTM_END_AUG_PAIR_TRAIN_DATA_NEGATIVE_DIR, random_time_delta, num_frames=6, sample_mult=60, direction='backward', flare_class=classes)\n",
    "    SaveAIANAugmentedDataFromDF(class_df, LSTM_END_AUG_PAIR_TRAIN_DATA_POSITIVE_DIR, time_delta, num_frames=6, sample_mult=60, direction='backward', flare_class=classes)\n",
    "    \n",
    "    SaveAIANAugmentedDataFromDF(val_df, LSTM_END_AUG_PAIR_VAL_DATA_POSITIVE_DIR, time_delta, num_frames=6, sample_mult=1, direction='backward', flare_class=classes)\n",
    "    SaveAIANAugmentedDataFromDF(val_df, LSTM_END_AUG_PAIR_VAL_DATA_NEGATIVE_DIR, random_time_delta, num_frames=6, sample_mult=1, direction='backward', flare_class=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65fe4e6c-e2dc-4163-9863-b42107761cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAugmentedLSTMDataAllClass(class_df, val_split, classes='all'):\n",
    "    class_df = class_df.sample(frac=1).reset_index(drop=True)\n",
    "    event_num = len(class_df)\n",
    "    val_split_num = math.ceil(event_num*val_split)\n",
    "    val_df = class_df[-val_split_num:]\n",
    "    class_df = class_df[:-val_split_num]\n",
    "    time_delta = 0\n",
    "    random_time_delta = -60\n",
    "    num_frames = 6\n",
    "    \n",
    "    # p = Pool(8)\n",
    "    # p.apply_async(SaveAIAAllClassAugmentedDataFromDF, args=(class_df, LSTM_AUG_ALL_CLASS_PRIOR_TRAIN_DATA_DIR, time_delta, num_frames, 1, 'backward', classes))\n",
    "    # p.apply_async(SaveAIAAllClassAugmentedDataFromDF, args=(val_df, LSTM_AUG_ALL_CLASS_PRIOR_VAL_DATA_DIR, time_delta, num_frames, 1, 'backward', classes))\n",
    "    # p.apply_async(SaveAIAAllClassAugmentedDataFromDF, args=(class_df, f'{LSTM_AUG_ALL_CLASS_PRIOR_TRAIN_DATA_DIR}/N', random_time_delta, 6, 2, 'backward', 'C'))\n",
    "    # p.apply_async(SaveAIAAllClassAugmentedDataFromDF, args=(val_df, f'{LSTM_AUG_ALL_CLASS_PRIOR_VAL_DATA_DIR}/N', random_time_delta, 6, 1, 'backward', 'C'))\n",
    "    # p.close()\n",
    "    # p.join()\n",
    "    \n",
    "        \n",
    "        \n",
    "    SaveAIAAllClassAugmentedDataFromDF(class_df, LSTM_AUG_ALL_CLASS_PRIOR_TRAIN_DATA_DIR, time_delta, num_frames=num_frames, sample_mult=None, direction='backward', flare_class=classes)    \n",
    "    SaveAIAAllClassAugmentedDataFromDF(val_df, LSTM_AUG_ALL_CLASS_PRIOR_VAL_DATA_DIR, time_delta, num_frames=num_frames, sample_mult=1, direction='backward', flare_class=classes)\n",
    "    SaveAIANAugmentedDataFromDF(class_df, f'{LSTM_AUG_ALL_CLASS_PRIOR_TRAIN_DATA_DIR}/N', random_time_delta, num_frames=6, sample_mult=20, direction='backward', flare_class='M')\n",
    "    SaveAIANAugmentedDataFromDF(val_df, f'{LSTM_AUG_ALL_CLASS_PRIOR_VAL_DATA_DIR}/N', random_time_delta, num_frames=6, sample_mult=1, direction='backward', flare_class='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07dd4569-6276-4875-bcc3-187db000bd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CreateResnetData(classes=FLARE_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "927fc9ca-641f-485e-9eca-773ea0932f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateLSTMData(classes=FLARE_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "967f2ab3-7461-46ff-98a1-4afff9c9833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreatePairData(classes=FLARE_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9e20786-96c5-47ca-8d6f-2eafdec99281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateAugmentedData(classes=FLARE_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "346c1ab8-39c2-4685-86c0-e5f44adef6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateAugmentedLSTMData(classes=FLARE_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "181cd3d1-250e-4c83-9531-622ee68e93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateEndAugmentedLSTMData(classes=FLARE_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be122880-2718-4da2-9e21-d002b31595bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "X\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "CreateAugmentedLSTMDataAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf926f36-0eca-4927-bace-556445d04d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 330513 val data: 24036 test data: 0\n"
     ]
    }
   ],
   "source": [
    "total_train_data = 0\n",
    "total_test_data = 0\n",
    "total_val_data = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(LSTM_AUG_ALL_CLASS_PRIOR_TRAIN_DATA_DIR):\n",
    "    total_train_data += len(files)\n",
    "    \n",
    "for subdir, dirs, files in os.walk(LSTM_AUG_ALL_CLASS_PRIOR_VAL_DATA_DIR):\n",
    "    total_val_data += len(files)\n",
    "    \n",
    "# for subdir, dirs, files in os.walk(TEST_DATA_POSITIVE_DIR):\n",
    "#     total_test_data += len(files)\n",
    "    \n",
    "# for subdir, dirs, files in os.walk(TEST_DATA_NEGATIVE_DIR):\n",
    "#     total_test_data += len(files)\n",
    "    \n",
    "# for subdir, dirs, files in os.walk(AUG_PAIR_VAL_DATA_POSITIVE_DIR):\n",
    "#     total_val_data += len(files)\n",
    "    \n",
    "# for subdir, dirs, files in os.walk(AUG_PAIR_VAL_DATA_NEGATIVE_DIR):\n",
    "#     total_val_data += len(files)\n",
    "    \n",
    "print(f'train data: {total_train_data} val data: {total_val_data} test data: {total_test_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cecf2-9aff-4176-896c-3e645ade5edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
